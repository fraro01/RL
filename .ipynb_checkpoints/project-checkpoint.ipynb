{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43531c3c",
   "metadata": {},
   "source": [
    "# Investment Strategies with Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35718b0",
   "metadata": {},
   "source": [
    "The aim of this Jupyter Notebook is to apply the Deep Q-Network (DQN) algorithm in the world of investment.\\\n",
    "In order to do so, the main essential thing is to create an environment in which to operate. The environment must have all the methods and characteristics that all Gymnasium environments have.\\\n",
    "In the specific, given a market that we want to analyze as input, it must returns all the relevant features of a typical environment, such as the actions that we can take, the observation space, terminated or truncated, and more importantly the **REWARD** signal that must indicate to the agent what we want to achieve.\\\n",
    "Once we have that, we connect it to the DQN algorithm, after some training we will evaluate the reward that it achieves in order to understand the goodness of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e799d6",
   "metadata": {},
   "source": [
    "It is important to point out that [Gymnasium](https://gymnasium.farama.org/) do not have any predefined trading environment, all we have are some projects carried out 'unofficially', thta often lack of corrcetness and rigorous methoods or documentation.\n",
    "\n",
    "* [gym-anytrading](https://github.com/AminHP/gym-anytrading), repository GitHub for a possible trading environment.\n",
    "* [tensortrade](https://github.com/tensortrade-org/tensortrade), Python library for reinforcement learning applied in trading.\n",
    "* [q-trader](https://github.com/edwardhdlu/q-trader?tab=readme-ov-file), application of reinforcement learning in the stock market.\n",
    "\n",
    "These three, seem to be the most relevant sources we can find on the web, but as already noted, they lack of proper documentation, and rigorous method.\n",
    "\n",
    "In any case, the application of RL techniques in investments is already quite widespread as mentioned by this article [link](https://medium.com/ibm-data-ai/reinforcement-learning-the-business-use-case-part-2-c175740999), where prestigious firms suchs IBM or J.P. Morgan, already apply some of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20569071",
   "metadata": {},
   "source": [
    "The first thing to do, is the design of the environment, and I have designed my own, as shown below, it is contained in a script called `tradingenv`, that contains our environment class, `TradingEnv`.\\\n",
    "It has all the methods that make it compliant to the Gym environments. But the most relevant thing to understand is how the **reward** signal is defined, as shown here:\n",
    "\n",
    "$\n",
    "     reward = (cash_{t+1} + shares_{t+1} \\cdot p_{t+1}) - (cash_{t} + shares_{t} \\cdot p_{t}) \n",
    "$\n",
    "\n",
    "where:\n",
    "* $reward$, indicates the reward obtained after taking the action at time-step $t$\n",
    "* $cash_{t}$, indicates the amount of cash held at time step $t$\n",
    "* $share_{t}$, indicates the amount of share held at time step $t$\n",
    "* $p_{t}$, indicates the price of a single share at time step $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4efe8",
   "metadata": {},
   "source": [
    "## Implementation with the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48030a4d",
   "metadata": {},
   "source": [
    "import of all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48194505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of our defined environment\n",
    "#YOU MUST ALSO HAVE THE SCRIPT \"tradingenv.py\"\n",
    "from tradingenv import TradingEnv\n",
    "\n",
    "import torch #for Neural Networks\n",
    "import numpy as np #for numerical operations\n",
    "import random #for random number generation\n",
    "import joblib #for saving and loading the replay memory\n",
    "import matplotlib.pyplot as plt #for plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb060ce",
   "metadata": {},
   "source": [
    "example of how to use the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" example of how to use the described environment\n",
    "env = TradingEnv(\"AAPL\", \"1d\", sliding_window=10, start_date=\"2020-01-01\", end_date=\"2021-01-01\", initial_capital=10000)\n",
    "obs, info = env.reset()\n",
    "action = env.action_space.sample()  # es. 0=buy,1=hold,2=sell\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e04a9",
   "metadata": {},
   "source": [
    "Instantiaion of the trading environment with the selected stock ticker of interest.\\\n",
    "Note, you can select the granularity as the interval parameter of the `yfinance` library, representing the time scale over which you want to operate.\n",
    "supported time scale: \n",
    "\n",
    "#### Short-time trading (intraday)\n",
    "\n",
    "* `1m` – 1 minute\n",
    "* `2m` – 2 minutes\n",
    "* `5m` – 5 minutes\n",
    "* `15m` – 15 minutes\n",
    "* `30m` – 30 minutes\n",
    "* `60m` – 60 minutes\n",
    "* `90m` – 90 minutes\n",
    "* `1h` – 1 hour\n",
    "\n",
    "#### Long-time trading (daily and others)\n",
    "* `1d` – 1 day\n",
    "* `5d` – 5 days\n",
    "* `1wk` – 1 week\n",
    "* `1mo` – 1 month\n",
    "* `3mo` – 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c0e08",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TradingEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m TradingEnv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAPL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1d\u001b[39m\u001b[38;5;124m\"\u001b[39m, sliding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, start_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, end_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2020-05-06\u001b[39m\u001b[38;5;124m\"\u001b[39m, initial_capital\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30000\u001b[39m)\n\u001b[0;32m      3\u001b[0m state_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      4\u001b[0m action_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TradingEnv' is not defined"
     ]
    }
   ],
   "source": [
    "#instatiate the environmentS\n",
    "env = TradingEnv(\"AAPL\", \"1d\", sliding_window=10, start_date=\"2020-01-01\", end_date=\"2020-05-06\", initial_capital=30000)\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State Space: ', state_size)\n",
    "print('Action Space: ', action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12368f",
   "metadata": {},
   "source": [
    "Show what we are analyzing, the time series of interest, note that we use as prices the closing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1675fa",
   "metadata": {},
   "source": [
    "We fix the seed of the random number generator in order to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seeds for reproducibility of results\n",
    "seed = 34\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27cbb5",
   "metadata": {},
   "source": [
    "Baseline random policy, to be used as reference, in order to understan whether we are actually learning smoething"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pi(state):\n",
    "    # selects an action uniformly at random\n",
    "    # from the environment's action space.\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a2d8e",
   "metadata": {},
   "source": [
    "if available on the current calculator, we take advantage of the **GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the backend device to MPS, if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"cuda\") #mps\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321d462",
   "metadata": {},
   "source": [
    "Experience Replay Buffer, for the creation of semy-independent experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a973c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the structured dtype for an experience tuple\n",
    "experience_type = np.dtype([\n",
    "    ('state',      np.float32, state_size),   # current state\n",
    "    ('action',     np.int8),                  # action taken\n",
    "    ('reward',     np.float32),               # reward received\n",
    "    ('next_state', np.float32, state_size),   # next state\n",
    "    ('failure',    np.int8)                   # terminal flag (1 if done)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the replay memory size hyperparameter\n",
    "memory_size = 100000\n",
    "\n",
    "# Create the replay memory\n",
    "replay_memory = {\n",
    "    'size': memory_size,\n",
    "    'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "    'index': 0,\n",
    "    'entries': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e758b",
   "metadata": {},
   "source": [
    "Function to store a new experience in the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_experience(experience):\n",
    "    # store the experience in the buffer\n",
    "    replay_memory['buffer'][replay_memory['index']] = experience\n",
    "\n",
    "    # update the number of experiences in the buffer\n",
    "    replay_memory['entries'] = min(replay_memory['entries'] + 1, replay_memory['size'])\n",
    "\n",
    "    # update index, if the memory is full, start from the begging\n",
    "    replay_memory['index'] += 1\n",
    "    replay_memory['index'] = replay_memory['index'] % replay_memory['size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748a0f7",
   "metadata": {},
   "source": [
    "Function to sample a mini-batch of experiences from the replay buffer defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for sampling experiences\n",
    "batch_size = 32\n",
    "\n",
    "# function to sample a batch of experiences from the replay memory\n",
    "def sample_experiences():\n",
    "\n",
    "    # select uniformly at random a batch of experiences from the memory\n",
    "    idxs = np.random.choice(range(replay_memory['entries']), batch_size, replace=False)\n",
    "\n",
    "    # return the batch of experiences\n",
    "    experiences = replay_memory['buffer'][idxs]\n",
    "\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdbeee",
   "metadata": {},
   "source": [
    "### Neural Network architecture, it outputs the $Q(s,a)$ for all possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for the neural network architecture to be tweaked\n",
    "first_hidden_layer= 512\n",
    "second_hidden_layer= 128\n",
    "\n",
    "def create_network():\n",
    "\n",
    "      # Define a deep neural network using Sequential:\n",
    "      # Each layer feeds directly into the next one.\n",
    "      dnn = torch.nn.Sequential( \n",
    "            # First fully connected layer maps state inputs to 512 hidden units\n",
    "            torch.nn.Linear(state_size[0], first_hidden_layer),\n",
    "            \n",
    "            # ReLU activation introduces nonlinearity\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Second fully connected layer (hidden layer with 128 units)\n",
    "            torch.nn.Linear(first_hidden_layer, second_hidden_layer),\n",
    "            \n",
    "            # Another ReLU activation\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Output layer: one unit per possible action\n",
    "            # Produces Q-values\n",
    "            torch.nn.Linear(second_hidden_layer, action_size)\n",
    "      )\n",
    "    \n",
    "      # Return the constructed model\n",
    "      return dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67396881",
   "metadata": {},
   "source": [
    "Creation of two separated networks (in terms of weights), in order to adress the isse of **identically distributed** targets.\n",
    "\n",
    "$\\displaystyle L(\\theta) = E_{(s,a) \\sim U(D)} \\left[ ( r + \\gamma \\underset{a}{\\text{ max }} Q(s',a;\\theta^{-}) - Q(s,a;\\theta))^2 \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_q = create_network() # online q-network for the prediction\n",
    "target_q = create_network() # target network for the Ground-Truth Q-value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78cfe",
   "metadata": {},
   "source": [
    "Optimizer only for the online network, since is the onnliy one to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.007\n",
    "optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1f37e",
   "metadata": {},
   "source": [
    "The target network is not trained, but sometimes we need to **update its weights to match the online network weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target():\n",
    "    # copy the parameters from the online model to the target model\n",
    "    for target, online in zip(target_q.parameters(), online_q.parameters()):\n",
    "        target.data.copy_(online.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb6056",
   "metadata": {},
   "source": [
    "At the end of training the online network will be used to implement the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ca563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_pi(state):\n",
    "    # convert the state into a tensor\n",
    "    state = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "    # compute Q-values from the network\n",
    "    q_values = online_q(state).detach().numpy().squeeze()\n",
    "\n",
    "    # select greedy action\n",
    "    action = int(np.argmax(q_values))\n",
    "\n",
    "    # return the action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b6819",
   "metadata": {},
   "source": [
    "Function to evaluate a policy, it return the average reward obtained over a number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pi, episodes=1):\n",
    "\n",
    "     # collect total rewards per episode\n",
    "    rewards = []\n",
    "\n",
    "    # loop over episodes\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # reset the environment\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # run an episode\n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            state, reward, terminal, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminal or truncated\n",
    "\n",
    "        # store the total reward    \n",
    "        rewards.append(total_reward)\n",
    "            \n",
    "    # return the average reward over the episodes        \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a44ef",
   "metadata": {},
   "source": [
    "Optimizer, for computing the loss and the backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87704058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "def optimize():\n",
    "\n",
    "    # sample a batch of experiences\n",
    "    batch = sample_experiences()\n",
    "    \n",
    "    # prepare the experience as tensors\n",
    "    states      = torch.from_numpy(batch['state'].copy()).float()    \n",
    "    actions     = torch.from_numpy(batch['action'].copy()).long()   \n",
    "    rewards     = torch.from_numpy(batch['reward'].copy()).float()    \n",
    "    next_states = torch.from_numpy(batch['next_state'].copy()).float() \n",
    "    failures    = torch.from_numpy(batch['failure'].copy()).float()\n",
    "\n",
    "    # get the values of the Q-function at next state from the \"target\" network \n",
    "    # remember to detach, we need to treat these values as constants \n",
    "    q_target_next = target_q(next_states).detach()\n",
    "    \n",
    "    # get the max value \n",
    "    max_q_target_next = q_target_next.max(1)[0]\n",
    "\n",
    "    # one important step, often overlooked, is to ensure \n",
    "    # that failure states are grounded to zero\n",
    "    max_q_target_next *= (1 - failures.float())\n",
    "\n",
    "    # calculate the target \n",
    "    target = rewards + gamma * max_q_target_next\n",
    "\n",
    "    # finally, we get the current estimate of Q(s,a)\n",
    "    # here we query the current \"online\" network\n",
    "    q_online_current = torch.gather(online_q(states), 1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # create the errors\n",
    "    td_error = target - q_online_current\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = td_error.pow(2).mean()\n",
    "\n",
    "    # backward pass: compute the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c5115",
   "metadata": {},
   "source": [
    "Hyperparameters to be set for the exploration straetgies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39038a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decay parameters (max, min, steps)\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay_steps = 10000\n",
    "\n",
    "# generate epsilons\n",
    "epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "   \n",
    "# normalize epsilons \n",
    "epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "    \n",
    "# scale  epsilons to the desired range\n",
    "epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c384cb",
   "metadata": {},
   "source": [
    "Chosen exploration strategy.\\\n",
    "Based on the picked $\\epsilon$, we alternate a random action selection and the greedy selection (greedy based on the value of the $Q(s,a,\\theta)$ returned by our neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9794a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def epsilon_greedy(state, step):\n",
    "    # get the epsilon value    \n",
    "    epsilon = epsilons[step] if step < epsilon_decay_steps else epsilon_min\n",
    "\n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        action = random_pi(state)\n",
    "\n",
    "    # Exploitation\n",
    "    else:\n",
    "        action = dqn_pi(state)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c3170",
   "metadata": {},
   "source": [
    "#### Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(memory_start_size, target_update_steps, max_episodes):\n",
    "    \n",
    "    # create a score tracker for statistic purposes\n",
    "    scores = []\n",
    "    \n",
    "    # counter for the number of steps \n",
    "    step = 0\n",
    "\n",
    "    # update the target model with the online one\n",
    "    update_target()\n",
    "                   \n",
    "    # train until the maximum number of episodes\n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        # reset the environment before starting the episode\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # interact with the environment until the episode is done\n",
    "        while not done:\n",
    "                    \n",
    "            # select the action using the exploration policy\n",
    "            action = epsilon_greedy(state, step)\n",
    "\n",
    "            # perform the selected action\n",
    "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "            done = terminal or truncated\n",
    "            failure = terminal and not truncated\n",
    "\n",
    "            # store the experience into the replay buffer\n",
    "            experience = (state, action, reward, next_state, failure)\n",
    "            store_experience(experience)\n",
    "    \n",
    "            # optimize the online model after the replay buffer is large enough\n",
    "            if replay_memory['entries'] > memory_start_size:\n",
    "                optimize()\n",
    "                 \n",
    "                # sometimes, synchronize the target model with the online model\n",
    "                if step % target_update_steps == 0:\n",
    "                    update_target()\n",
    "                \n",
    "            # update current state to next state\n",
    "            state = next_state\n",
    "\n",
    "            # update the step counter\n",
    "            step += 1\n",
    "\n",
    "        # After each episode, evaluate the policy\n",
    "        score = evaluate(dqn_pi, episodes=10)\n",
    "\n",
    "        # store the score in the tracker\n",
    "        scores.append(score)\n",
    "\n",
    "        # print some informative logging \n",
    "        message = 'Episode {:03}, score {:05.1f}'\n",
    "        message = message.format(episode+1, score)\n",
    "        print(message, end='\\r', flush=True)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9c2f2",
   "metadata": {},
   "source": [
    "Apply the DQN to out `TradingEnv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c240f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "memory_start_size = 1000\n",
    "max_episodes = 5 #TODO! 100\n",
    "target_update_steps = 10\n",
    "\n",
    "# run the DQN algorithm\n",
    "dqn(memory_start_size, target_update_steps, max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706bff6",
   "metadata": {},
   "source": [
    "Definition of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(max_episodes):\n",
    "\n",
    "    global online_q, target_q, optimizer, replay_memory, epsilons\n",
    "\n",
    "    # List of random seeds to test algorithm stability\n",
    "    seeds = (12, 34, 56, 78, 90)\n",
    "\n",
    "    # Container to collect all experiment results\n",
    "    results = []\n",
    "\n",
    "    # Run an independent training experiment per seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Experiment seed: \", seed)\n",
    "\n",
    "         # Set all relevant random seeds for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        # create online and target models\n",
    "        online_q = create_network()\n",
    "        target_q = create_network()\n",
    "        optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)\n",
    "\n",
    "        # create the replay memory\n",
    "        replay_memory = {\n",
    "            'size': memory_size,\n",
    "            'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "            'index': 0,\n",
    "            'entries': 0\n",
    "        }\n",
    "\n",
    "        # create the epsilon values\n",
    "        epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "        epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "        epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min\n",
    "\n",
    "        # train the network    \n",
    "        scores = dqn(memory_start_size, target_update_steps, max_episodes)\n",
    "        \n",
    "        # smooth the result using a sliding window\n",
    "        sliding_windows = 25\n",
    "        scores = np.convolve(scores, np.ones(sliding_windows)/sliding_windows, mode='valid')\n",
    "                \n",
    "        # collect the results\n",
    "        results.append(scores)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # calculate max, min and average scores among experiments\n",
    "    max_score = np.max(results, axis=0).T\n",
    "    min_score = np.min(results, axis=0).T\n",
    "    mean_score = np.mean(results, axis=0).T\n",
    "\n",
    "    # prepare the results\n",
    "    experiment_results = {\n",
    "        'max_score': max_score,\n",
    "        'min_score': min_score,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "    # save permanently\n",
    "    joblib.dump(experiment_results, '../dqn_results.joblib');\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91ef7",
   "metadata": {},
   "source": [
    "Run of the experiment on our environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experiment setup and hyperparameters\n",
    "gamma = 0.99;               # discount factor\n",
    "learning_rate = 0.001;      # step size for the optimizer\n",
    "batch_size = 512;          # number of experiences per batch\n",
    "epochs = 8;                # optimization steps per batch\n",
    "epsilon = 0.5               # esploration vs exploitation parameter\n",
    "first_hidden_layer = 256;   # size of the first hidden layer\n",
    "second_hidden_layer = 128;  # size of the second hidden layer\n",
    "\n",
    "# Run the experiment\n",
    "dqn_results = experiment(max_episodes=1) #TODO! 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_random(max_episodes):\n",
    "\n",
    "    # List of random seeds to test algorithm stability\n",
    "    seeds = (12, 34, 56, 78, 90)\n",
    "\n",
    "    # Container to collect all experiment results\n",
    "    results = []\n",
    "\n",
    "    # Run an independent training experiment per seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Experiment seed: \", seed)\n",
    "\n",
    "         # Set all relevant random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        # train the network    \n",
    "        scores = evaluate(random_pi, episodes=max_episodes)\n",
    "        \n",
    "        # smooth the result using a sliding window\n",
    "        sliding_windows = 25\n",
    "        scores = np.convolve(scores, np.ones(sliding_windows)/sliding_windows, mode='valid')\n",
    "                \n",
    "        # collect the results\n",
    "        results.append(scores)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # calculate max, min and average scores among experiments\n",
    "    max_score = np.max(results, axis=0).T\n",
    "    min_score = np.min(results, axis=0).T\n",
    "    mean_score = np.mean(results, axis=0).T\n",
    "\n",
    "    # prepare the results\n",
    "    experiment_results = {\n",
    "        'max_score': max_score,\n",
    "        'min_score': min_score,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pi_results = experiment_random(max_episodes=1) #TODO! 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_pi_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f61242",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(random_pi_results['mean_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dqn_results['mean_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed4c39",
   "metadata": {},
   "source": [
    "TODO! PLOT THE COMPARISON BETWEEN THE RANDOM POLCY AND THE DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c814b3",
   "metadata": {},
   "source": [
    "Plotting of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cd4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.title('DQN vs Random Policy')\n",
    "plt.ylabel('Average reward per episode [$]')\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "dqn_episodes = range(len(dqn_results['max_score']))\n",
    "random_pi_episodes = range(len(random_pi_results['max_score']))\n",
    "\n",
    "plt.plot(random_pi_results['max_score'], 'y', linewidth=1, label=\"random_pi\")\n",
    "plt.plot(random_pi_results['min_score'], 'y', linewidth=1)\n",
    "plt.plot(random_pi_results['mean_score'], 'y', linewidth=2)\n",
    "plt.fill_between(random_pi_episodes, random_pi_results['min_score'], random_pi_results['max_score'], facecolor='y', alpha=0.3)\n",
    "\n",
    "plt.plot(dqn_results['max_score'], 'b', linewidth=1, label=\"DQN\")\n",
    "plt.plot(dqn_results['min_score'], 'b', linewidth=1)\n",
    "plt.plot(dqn_results['mean_score'], 'b', linewidth=2)\n",
    "plt.fill_between(dqn_episodes, dqn_results['min_score'], dqn_results['max_score'], facecolor='b', alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d911f9",
   "metadata": {},
   "source": [
    "## Conclusions, limitations and further improvements\n",
    "\n",
    "one single share to be sold or bought per actoin,\\\n",
    "markovian market\\\n",
    "see my paper\\\n",
    "see my fft for finance github, percentage variation of prices\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c47c83",
   "metadata": {},
   "source": [
    "# important things\n",
    "\n",
    "1. put the % variation of prices\n",
    "2. keep the environment class as simple as possible, nothing superfluo\n",
    "7. implement a simple implementation of what to buy and sell, such as just ONE stock, a portion of it?\n",
    "8. evebn the action space must be super simple.\n",
    "9. fai una comparazione importantissima ta quello implmementato online dal tizio, le notes di Berta e quello che voglio fare io!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b2e9c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
