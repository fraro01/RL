{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48194505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of our defined environment\n",
    "from environment import TradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" example of how to use the described environment\n",
    "env = TradingEnv(\"AAPL\", \"1d\", sliding_window=10, start_date=\"2020-01-01\", end_date=\"2021-01-01\")\n",
    "obs, info = env.reset()\n",
    "action = env.action_space.sample()  # es. 0=buy,1=hold,2=sell\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingEnv(\"AAPL\", \"1d\", sliding_window=10, start_date=\"2020-01-01\", end_date=\"2020-05-06\")\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State Space: ', state_size)\n",
    "print('Action Space: ', action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# set the seeds for reproducibility of results\n",
    "seed = 34\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "env.np_random = np.random.Generator(np.random.PCG64(seed))\n",
    "\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pi(state):\n",
    "    # selects an action uniformly at random\n",
    "    # from the environment's action space.\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the backend device to MPS, if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"gpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a973c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the structured dtype for an experience tuple\n",
    "experience_type = np.dtype([\n",
    "    ('state',      np.float32, state_size),   # current state\n",
    "    ('action',     np.int8),                  # action taken\n",
    "    ('reward',     np.float32),               # reward received\n",
    "    ('next_state', np.float32, state_size),   # next state\n",
    "    ('failure',    np.int8)                   # terminal flag (1 if done)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the replay memory size\n",
    "memory_size = 100000; \n",
    "\n",
    "# Create the replay memory\n",
    "replay_memory = {\n",
    "    'size': memory_size,\n",
    "    'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "    'index': 0,\n",
    "    'entries': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_experience(experience):\n",
    "    # store the experience in the buffer\n",
    "    replay_memory['buffer'][replay_memory['index']] = experience\n",
    "\n",
    "    # update the number of experiences in the buffer\n",
    "    replay_memory['entries'] = min(replay_memory['entries'] + 1, replay_memory['size'])\n",
    "\n",
    "    # update index, if the memory is full, start from the begging\n",
    "    replay_memory['index'] += 1\n",
    "    replay_memory['index'] = replay_memory['index'] % replay_memory['size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for sampling experiences\n",
    "batch_size = 32\n",
    "\n",
    "# function to sample a batch of experiences from the replay memory\n",
    "def sample_experiences():\n",
    "\n",
    "    # select uniformly at random a batch of experiences from the memory\n",
    "    idxs = np.random.choice(range(replay_memory['entries']), batch_size, replace=False)\n",
    "\n",
    "    # return the batch of experiences\n",
    "    experiences = replay_memory['buffer'][idxs]\n",
    "\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "first_hidden_layer= 512\n",
    "second_hidden_layer= 128\n",
    "\n",
    "def create_network():\n",
    "\n",
    "      # Define a deep neural network using Sequential:\n",
    "      # Each layer feeds directly into the next one.\n",
    "      dnn = torch.nn.Sequential( \n",
    "            # First fully connected layer maps state inputs to 512 hidden units\n",
    "            torch.nn.Linear(state_size[0], first_hidden_layer),\n",
    "            \n",
    "            # ReLU activation introduces nonlinearity\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Second fully connected layer (hidden layer with 128 units)\n",
    "            torch.nn.Linear(first_hidden_layer, second_hidden_layer),\n",
    "            \n",
    "            # Another ReLU activation\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Output layer: one unit per possible action\n",
    "            # Produces Q-values\n",
    "            torch.nn.Linear(second_hidden_layer, action_size)\n",
    "      )\n",
    "    \n",
    "      # Return the constructed model\n",
    "      return dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_q = create_network()\n",
    "target_q = create_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.007\n",
    "optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target():\n",
    "    # copy the parameters from the online model to the target model\n",
    "    for target, online in zip(target_q.parameters(), online_q.parameters()):\n",
    "        target.data.copy_(online.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ca563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_pi(state):\n",
    "    # convert the state into a tensor\n",
    "    state = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "    # compute Q-values from the network\n",
    "    q_values = online_q(state).detach().numpy().squeeze()\n",
    "\n",
    "    # select greedy action\n",
    "    action = int(np.argmax(q_values))\n",
    "\n",
    "    # return the action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pi, episodes=1):\n",
    "\n",
    "     # collect total rewards per episode\n",
    "    rewards = []\n",
    "\n",
    "    # loop over episodes\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # reset the environment\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # run an episode\n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            state, reward, terminal, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminal or truncated\n",
    "\n",
    "        # store the total reward    \n",
    "        rewards.append(total_reward)\n",
    "            \n",
    "    # return the average reward over the episodes        \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87704058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "def optimize():\n",
    "\n",
    "    # sample a batch of experiences\n",
    "    batch = sample_experiences()\n",
    "    \n",
    "    # prepare the experience as tensors\n",
    "    states      = torch.from_numpy(batch['state'].copy()).float()    \n",
    "    actions     = torch.from_numpy(batch['action'].copy()).long()   \n",
    "    rewards     = torch.from_numpy(batch['reward'].copy()).float()    \n",
    "    next_states = torch.from_numpy(batch['next_state'].copy()).float() \n",
    "    failures    = torch.from_numpy(batch['failure'].copy()).float()\n",
    "\n",
    "    # get the values of the Q-function at next state from the \"target\" network \n",
    "    # remember to detach, we need to treat these values as constants \n",
    "    q_target_next = target_q(next_states).detach()\n",
    "    \n",
    "    # get the max value \n",
    "    max_q_target_next = q_target_next.max(1)[0]\n",
    "\n",
    "    # one important step, often overlooked, is to ensure \n",
    "    # that failure states are grounded to zero\n",
    "    max_q_target_next *= (1 - failures.float())\n",
    "\n",
    "    # calculate the target \n",
    "    target = rewards + gamma * max_q_target_next\n",
    "\n",
    "    # finally, we get the current estimate of Q(s,a)\n",
    "    # here we query the current \"online\" network\n",
    "    q_online_current = torch.gather(online_q(states), 1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # create the errors\n",
    "    td_error = target - q_online_current\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = td_error.pow(2).mean()\n",
    "\n",
    "    # backward pass: compute the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39038a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decay parameters (max, min, steps)\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay_steps = 10000\n",
    "\n",
    "# generate epsilons\n",
    "epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "   \n",
    "# normalize epsilons \n",
    "epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "    \n",
    "# scale  epsilons to the desired range\n",
    "epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9794a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def epsilon_greedy(state, step):\n",
    "    # get the epsilon value    \n",
    "    epsilon = epsilons[step] if step < epsilon_decay_steps else epsilon_min\n",
    "\n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        action = random_pi(state)\n",
    "\n",
    "    # Exploitation\n",
    "    else:\n",
    "        action = dqn_pi(state)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(memory_start_size, target_update_steps, max_episodes):\n",
    "    \n",
    "    # create a score tracker for statistic purposes\n",
    "    scores = []\n",
    "    \n",
    "    # counter for the number of steps \n",
    "    step = 0\n",
    "\n",
    "    # update the target model with the online one\n",
    "    update_target()\n",
    "                   \n",
    "    # train until the maximum number of episodes\n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        # reset the environment before starting the episode\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # interact with the environment until the episode is done\n",
    "        while not done:\n",
    "                    \n",
    "            # select the action using the exploration policy\n",
    "            action = epsilon_greedy(state, step)\n",
    "\n",
    "            # perform the selected action\n",
    "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "            done = terminal or truncated\n",
    "            failure = terminal and not truncated\n",
    "\n",
    "            # store the experience into the replay buffer\n",
    "            experience = (state, action, reward, next_state, failure)\n",
    "            store_experience(experience)\n",
    "    \n",
    "            # optimize the online model after the replay buffer is large enough\n",
    "            if replay_memory['entries'] > memory_start_size:\n",
    "                optimize()\n",
    "                 \n",
    "                # sometimes, synchronize the target model with the online model\n",
    "                if step % target_update_steps == 0:\n",
    "                    update_target()\n",
    "                \n",
    "            # update current state to next state\n",
    "            state = next_state\n",
    "\n",
    "            # update the step counter\n",
    "            step += 1\n",
    "\n",
    "        # After each episode, evaluate the policy\n",
    "        score = evaluate(dqn_pi, episodes=10)\n",
    "\n",
    "        # store the score in the tracker\n",
    "        scores.append(score)\n",
    "\n",
    "        # print some informative logging \n",
    "        message = 'Episode {:03}, score {:05.1f}'\n",
    "        message = message.format(episode+1, score)\n",
    "        print(message, end='\\r', flush=True)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c240f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "memory_start_size = 1000\n",
    "max_episodes = 25 #TODO! 100\n",
    "target_update_steps = 10\n",
    "\n",
    "# run the DQN algorithm\n",
    "dqn(memory_start_size, target_update_steps, max_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def experiment(max_episodes):\n",
    "\n",
    "    global online_q, target_q, optimizer, replay_memory, epsilons\n",
    "\n",
    "    # List of random seeds to test algorithm stability\n",
    "    seeds = (12, 34, 56, 78, 90)\n",
    "\n",
    "    # Container to collect all experiment results\n",
    "    results = []\n",
    "\n",
    "    # Run an independent training experiment per seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Experiment seed: \", seed)\n",
    "\n",
    "         # Set all relevant random seeds for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        # create online and target models\n",
    "        online_q = create_network()\n",
    "        target_q = create_network()\n",
    "        optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)\n",
    "\n",
    "        # create the replay memory\n",
    "        replay_memory = {\n",
    "            'size': memory_size,\n",
    "            'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "            'index': 0,\n",
    "            'entries': 0\n",
    "        }\n",
    "\n",
    "        # create the epsilon values\n",
    "        epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "        epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "        epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min\n",
    "\n",
    "        # train the network    \n",
    "        scores = dqn(memory_start_size, target_update_steps, max_episodes)\n",
    "        \n",
    "        # smooth the result using a sliding window\n",
    "        sliding_windows = 25\n",
    "        scores = np.convolve(scores, np.ones(sliding_windows)/sliding_windows, mode='valid')\n",
    "                \n",
    "        # collect the results\n",
    "        results.append(scores)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # calculate max, min and average scores among experiments\n",
    "    max_score = np.max(results, axis=0).T\n",
    "    min_score = np.min(results, axis=0).T\n",
    "    mean_score = np.mean(results, axis=0).T\n",
    "\n",
    "    # prepare the results\n",
    "    experiment_results = {\n",
    "        'max_score': max_score,\n",
    "        'min_score': min_score,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "    # save permanently\n",
    "    joblib.dump(experiment_results, '../dqn_results.joblib');\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experiment setup and hyperparameters\n",
    "gamma = 0.99;               # discount factor\n",
    "learning_rate = 0.001;      # step size for the optimizer\n",
    "batch_size = 512;          # number of experiences per batch\n",
    "epochs = 8;                # optimization steps per batch\n",
    "epsilon = 0.5               # esploration vs exploitation parameter\n",
    "first_hidden_layer = 256;   # size of the first hidden layer\n",
    "second_hidden_layer = 128;  # size of the second hidden layer\n",
    "\n",
    "# Run the experiment\n",
    "dqn_results = experiment(max_episodes=1) #TODO! 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('DQN cumulative reward')\n",
    "plt.ylabel('Aggregated reward [$]')\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "episodes = range(len(dqn_results['max_score']))\n",
    "\n",
    "plt.plot(dqn_results['max_score'], 'b', linewidth=1, label=\"DQN\")\n",
    "plt.plot(dqn_results['min_score'], 'b', linewidth=1)\n",
    "plt.plot(dqn_results['mean_score'], 'b', linewidth=2)\n",
    "plt.fill_between(episodes, dqn_results['min_score'], dqn_results['max_score'], facecolor='b', alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcaddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "max_s = np.asarray(dqn_results['max_score'])\n",
    "min_s = np.asarray(dqn_results['min_score'])\n",
    "mean_s = np.asarray(dqn_results['mean_score'])\n",
    "\n",
    "episodes = np.arange(len(mean_s))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episodes, mean_s, label='DQN mean', linewidth=2)\n",
    "plt.fill_between(episodes, min_s, max_s, alpha=0.3, label='minâ€“max range')\n",
    "\n",
    "plt.title('DQN cumulative reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Aggregated reward [$]')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5654d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
