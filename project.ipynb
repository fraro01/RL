{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43531c3c",
   "metadata": {},
   "source": [
    "# Short-Term Investment Strategies with Deep Q-Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2c24f4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this Jupyter Notebook is to explore the application of **Deep Reinforcement Learning**, and in particular the **Deep Q-Network (DQN)** algorithm, in the domain of financial investments, with a focus on **short-term investment strategies**.\n",
    "\n",
    "The investment landscape can be broadly divided into two main categories.  \n",
    "1. **Long-term investments**, typically rely on macroeconomic factors, political events, corporate decisions, and geopolitical dynamics, and are often analyzed through [fundamental analysis](https://en.wikipedia.org/wiki/Fundamental_analysis).  \n",
    "2. **short-term investments**, which include trading activities, operate over much shorter time horizons and are commonly studied using mathematical and statistical models.\n",
    "\n",
    "Traditional approaches in quantitative finance for short-term price modeling include tools such as **ARIMA models**, **Geometric Brownian Motion (GBM)**, and model-based frameworks like the **Black–Scholes model**, (if you are interested, see my [arXiv paper here](https://arxiv.org/pdf/2510.27277)). While powerful, these methods rely on strong assumptions about market dynamics.\n",
    "\n",
    "In this notebook, we adopt a different perspective by leveraging **Artificial Intelligence**, in particular **Reinforcement Learning (RL)**.\\\n",
    "Instead of explicitly modeling price dynamics, we allow an agent to **learn a trading policy directly from interactions with the market environment**, guided by a reward signal.\n",
    "\n",
    "A crucial step in this process is the design of a suitable **trading environment**. The environment must comply with the standard **Gymnasium** interface and provide:\n",
    "- a well-defined **action space** (e.g.: buy, sell, hold),\n",
    "- an **observation space** representing the market state,\n",
    "- proper **termination and truncation conditions**,\n",
    "- and, most importantly, a **reward function** that encodes the trading objective.\n",
    "\n",
    "Once the environment is defined, it is coupled with a **DQN agent**, which is trained through repeated interactions with the market. Finally, the trained agent's policy is evaluated by analyzing the accumulated rewards compared to a random policy, in order to assess the effectiveness and robustness of the learned trading strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef29ad9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"trading_bot.JPG\" alt=\"trading bot image\" width=\"66%\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e799d6",
   "metadata": {},
   "source": [
    "## Current SOTA\n",
    "\n",
    "It is important to point out that [Gymnasium](https://gymnasium.farama.org/), which is a standardized API for RL ready-to-use environments, currently does not have any predefined trading environment, all we have are some projects carried out 'unofficially', that often lack of correctness and rigorous methoods or documentation.\n",
    "\n",
    "* [gym-anytrading](https://github.com/AminHP/gym-anytrading), repository GitHub for a possible trading environment.\n",
    "* [tensortrade](https://github.com/tensortrade-org/tensortrade), Python library for reinforcement learning applied in trading.\n",
    "* [q-trader](https://github.com/edwardhdlu/q-trader?tab=readme-ov-file), application of reinforcement learning in the stock market.\n",
    "\n",
    "These three, seem to be the most relevant sources we can find on the web, but as already noted, they lack of proper documentation and rigorous method.\n",
    "\n",
    "In any case, the application of RL techniques in investments is already quite widespread as mentioned by this article [link](https://medium.com/ibm-data-ai/reinforcement-learning-the-business-use-case-part-2-c175740999), where prestigious firms suchs IBM or J.P. Morgan, already apply some of these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20569071",
   "metadata": {},
   "source": [
    "## The *environment* and  *reward* signal\n",
    "\n",
    "Given the current SOTA available and having understood the main dynamics of a market stock, the first thing to do, is the design of our own environment.\\\n",
    "The Python script `tradingenv` contains our environment class, `TradingEnv()`. It could be used as a starting point for many other RL projects, since it contains a proper methodology with many comments that enhance the understanding of the development.\n",
    "\n",
    "TradingEnv has all the methods that make it compliant to the Gym environments. But the most relevant thing to understand is how the **reward** signal is defined, as shown here:\n",
    "\n",
    "$ reward = (cash_{t+1} + shares_{t+1} \\cdot p_{t+1}) - (cash_{t} + shares_{t} \\cdot p_{t}) $\n",
    "\n",
    "where:\n",
    "* $reward$, indicates the reward obtained after taking the action at time-step $t$\n",
    "* $cash_{t}$, indicates the amount of cash held at time step $t$\n",
    "* $share_{t}$, indicates the amount of shares held at time step $t$\n",
    "* $p_{t}$, indicates the price of a single share at time step $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48030a4d",
   "metadata": {},
   "source": [
    "## Development and Implementation\n",
    "\n",
    "import of all the necessary libraries, environment, numerical computations, NN and rendering...\\\n",
    "Make sure to have the script `tradingenv`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48194505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of our defined environment\n",
    "#YOU MUST ALSO HAVE THE SCRIPT \"tradingenv.py\"\n",
    "from tradingenv import TradingEnv\n",
    "\n",
    "import torch #for Neural Networks\n",
    "import numpy as np #for numerical operations\n",
    "import random #for random number generation\n",
    "import joblib #for saving and loading the replay memory\n",
    "import matplotlib.pyplot as plt #for plotting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb060ce",
   "metadata": {},
   "source": [
    "Example of how to use the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ac408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" example of how to use the described environment\n",
    "env = TradingEnv(ticker=\"AAPL\", granularity=\"1d\", sliding_window=7, start_date=\"2020-01-01\", end_date=\"2021-01-01\", initial_cash=10000, initial_share=5)\n",
    "obs, info = env.reset()\n",
    "action = env.action_space.sample()  # es. 0=buy,1=hold,2=sell\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e04a9",
   "metadata": {},
   "source": [
    "### Instantiation of the trading environment with the selected stock ticker of interest\n",
    "\n",
    "Note, you can select the granularity as the interval parameter of the `yfinance` [library](https://ranaroussi.github.io/yfinance/), representing the time scale over which you want to operate.\\\n",
    "Supported time scale: \n",
    "\n",
    "#### Short-time trading (intraday)\n",
    "\n",
    "* `1m` – 1 minute\n",
    "* `2m` – 2 minutes\n",
    "* `5m` – 5 minutes\n",
    "* `15m` – 15 minutes\n",
    "* `30m` – 30 minutes\n",
    "* `60m` – 60 minutes\n",
    "* `90m` – 90 minutes\n",
    "* `1h` – 1 hour\n",
    "\n",
    "#### Long-time trading (daily and others)\n",
    "* `1d` – 1 day\n",
    "* `5d` – 5 days\n",
    "* `1wk` – 1 week\n",
    "* `1mo` – 1 month\n",
    "* `3mo` – 3 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instatiate the environment\n",
    "env = TradingEnv(ticker=\"AAPL\", granularity=\"1d\", sliding_window=10, start_date=\"2022-01-01\", end_date=\"2025-01-01\", initial_cash=100000, initial_share=50)\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State Space: ', state_size)\n",
    "print('Action Space: ', action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4f11b",
   "metadata": {},
   "source": [
    "### Note that here we have some fundamental hypeparamenters to tweak\n",
    "\n",
    "* The `start_date` and `end_date` will generate the amount of data used for training in the period of interest, we must tweak them according to our purpose, but generally speaking they should cover a period of circa 5 years, in order to enclose possible financial market cycles.\n",
    "* The `slidinig_window` should be approximately 10 days with a maximum of 20, but for the sake of computational efficiency we have decided to set it to 7 days (keep in mind that it also depends on the type of trading the user want to perform).\n",
    "* The `granularity` hyperparameter as already mentioned is crucial, and depends on the type of trading the user wants to perform.\n",
    "* The `initial_cash` and `initial_share` combined, represent the agent initial wallet value, and are of crucial importance for the learning of behaviour, **we must give to the agent a proper amount of stock and cash available at the beginning, otherwise it will learn degenerative policy.**\\\n",
    "**Note that the two amounts should depend on the market considered.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef2928",
   "metadata": {},
   "source": [
    "### Observation Space: Relative Price Variations\n",
    "\n",
    "It is important to emphasize that the observation space is defined in terms of **percentage price variations** rather than **absolute price levels**.  \n",
    "This choice is fundamental from both a financial and a reinforcement learning perspective.\n",
    "\n",
    "Absolute prices are not directly comparable across assets or time periods: a price change of +1 has a very different meaning for an asset trading at 10 compared to one trading at 1,000. By using relative variations, the agent receives **scale-invariant information**, allowing it to learn patterns that are independent of nominal price levels.\n",
    "\n",
    "Moreover, financial markets are highly **non-stationary** in absolute terms, as prices tend to drift over time. Percentage variations, instead, exhibit more stable statistical properties, making them more suitable as inputs for learning algorithms and improving training stability.\n",
    "\n",
    "Finally, relative variations naturally align with the **economic objective of trading**, since profits and losses are inherently relative quantities. For these reasons, the observation space is constructed using relative price changes, enabling better generalization and a more meaningful learning process for the DQN agent.\n",
    "\n",
    "If you are interested, here is a similar project of mine where I needed stationarity in time series [Fast Fourier Transform in Finance](https://github.com/fraro01/Fourier-Transform-in-Finance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12368f",
   "metadata": {},
   "source": [
    "## What our agent is training on\n",
    "\n",
    "Show what we are analyzing, the time series of interest, note that we use as prices the closing prices given by `yfinance`. This behaviour can be changed by simpply tweaking the related hyperparameter from the `tradingenv.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172d402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of the TradingEnv class to show the data used for training\n",
    "env.show_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1675fa",
   "metadata": {},
   "source": [
    "We fix the seed of the random number generator in order to reproduce the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seeds for reproducibility of results\n",
    "seed = 34\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a2d8e",
   "metadata": {},
   "source": [
    "If available on the current calculator, we take advantage of the **GPU** computational power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the backend device to MPS or CUDA, if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# print the used device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321d462",
   "metadata": {},
   "source": [
    "## Experience Replay Buffer\n",
    "\n",
    "Experience Replay Buffer, for the creation of semi-independent experiences to train the agent, in order to adress the first issue given by: *i.i.d*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fc748",
   "metadata": {},
   "source": [
    "Definition of the single array of experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a973c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the structured dtype for an experience tuple, which will be stored in the replay memory\n",
    "experience_type = np.dtype([\n",
    "    ('state',      np.float32, state_size),   # current state, it must take the warray of the observation space\n",
    "    ('action',     np.int8),                  # action taken, it is JUST THE action we took\n",
    "    ('reward',     np.float32),               # reward received\n",
    "    ('next_state', np.float32, state_size),   # next state\n",
    "    ('failure',    np.int8)                   # terminal flag (1 if done)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fed524",
   "metadata": {},
   "source": [
    "Definition of the container of the arrays (replay buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c9f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the replay memory size hyperparameter\n",
    "memory_size = 100000 #TODO! ADJUST IT ACCORDINGLY, it waas set to 100000 at the beginning\n",
    "\n",
    "# Create the replay memory\n",
    "replay_memory = {\n",
    "    'size': memory_size,\n",
    "    'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "    'index': 0,\n",
    "    'entries': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e758b",
   "metadata": {},
   "source": [
    "Function to store a new experience in the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_experience(experience):\n",
    "    # store the experience in the buffer\n",
    "    replay_memory['buffer'][replay_memory['index']] = experience #we overwrite the experience at the current index, if the buffer is not full it will be empty, otherwise it will overwrite the oldest experience\n",
    "\n",
    "    # update the number of experiences in the buffer\n",
    "    replay_memory['entries'] = min(replay_memory['entries'] + 1, replay_memory['size']) # it cannot exceed the maximum size of the buffer\n",
    "\n",
    "    # update index, if the memory is full, start from the begging\n",
    "    replay_memory['index'] += 1\n",
    "    replay_memory['index'] = replay_memory['index'] % replay_memory['size'] #as long as 'index' is lower than 'size' it outputs 'index', then once they are equal it outputs 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9748a0f7",
   "metadata": {},
   "source": [
    "Function to sample a mini-batch of experiences from the replay buffer defined above.\\\n",
    "We will train the agent over these mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ea0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size for sampling experiences\n",
    "batch_size = 32 #hyperparameter to be tweaked TODO! it was 32\n",
    "\n",
    "# function to sample a batch of experiences from the replay memory\n",
    "def sample_experiences():\n",
    "\n",
    "    # select uniformly at random a batch of experiences from the memory\n",
    "    idxs = np.random.choice(range(replay_memory['entries']), batch_size, replace=False) #without replacement, we want unique experiences in the batch, if 'entries' is lower than 'batch_size' it will raise an error, but in that case we should not sample a batch of experiences, we should wait until we have enough experiences in the buffer\n",
    "\n",
    "    # return the batch of experiences\n",
    "    experiences = replay_memory['buffer'][idxs]\n",
    "\n",
    "    return experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdbeee",
   "metadata": {},
   "source": [
    "## Neural Network Architecture\n",
    "\n",
    "It will output the $Q(s,a)$ for all possible actions in the given state $s$, after the observation taken as input.\\\n",
    "Here many hyperparameters could be tweaked, such as the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed8465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters for the neural network architecture to be tweaked\n",
    "first_hidden_layer= 512\n",
    "second_hidden_layer= 128\n",
    "\n",
    "def create_network():\n",
    "\n",
    "      # Define a deep neural network using Sequential:\n",
    "      # Each layer feeds directly into the next one.\n",
    "      dnn = torch.nn.Sequential( \n",
    "            # First fully connected layer maps state inputs to 512 hidden units\n",
    "            torch.nn.Linear(state_size[0], first_hidden_layer),\n",
    "            \n",
    "            # ReLU activation introduces nonlinearity\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Second fully connected layer (hidden layer with 128 units)\n",
    "            torch.nn.Linear(first_hidden_layer, second_hidden_layer),\n",
    "            \n",
    "            # Another ReLU activation\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "            # Output layer: one unit per possible action\n",
    "            # Produces Q-values\n",
    "            torch.nn.Linear(second_hidden_layer, action_size)\n",
    "      )\n",
    "    \n",
    "      # Return the constructed model\n",
    "      return dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67396881",
   "metadata": {},
   "source": [
    "Creation of two separated networks (in terms of weights), in order to adress the second issue in *i.i.d.*, of *identically distributed* targets.\n",
    "\n",
    "$\\displaystyle L(\\theta) = E_{(s,a) \\sim U(D)} \\left[ ( r + \\gamma \\underset{a}{\\text{ max }} Q(s',a;\\theta^{-}) - Q(s,a;\\theta))^2 \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_q = create_network() # online q-network for the prediction\n",
    "target_q = create_network() # target network for the Ground-Truth Q-value estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d78cfe",
   "metadata": {},
   "source": [
    "Optimizer only for the online network, since is the only one to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3526b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.007\n",
    "optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1f37e",
   "metadata": {},
   "source": [
    "The target network is not trained, but sometimes we need to **update its weights to match the online network weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b9b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target():\n",
    "    # copy the parameters from the online model to the target model\n",
    "    for target, online in zip(target_q.parameters(), online_q.parameters()):\n",
    "        target.data.copy_(online.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236ff10",
   "metadata": {},
   "source": [
    "Optimizer, for computing the loss and the backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87704058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "def optimize():\n",
    "\n",
    "    # sample a batch of experiences\n",
    "    batch = sample_experiences()\n",
    "    \n",
    "    # prepare the experience as tensors\n",
    "    states      = torch.from_numpy(batch['state'].copy()).float()    \n",
    "    actions     = torch.from_numpy(batch['action'].copy()).long()   \n",
    "    rewards     = torch.from_numpy(batch['reward'].copy()).float()    \n",
    "    next_states = torch.from_numpy(batch['next_state'].copy()).float() \n",
    "    failures    = torch.from_numpy(batch['failure'].copy()).float()\n",
    "\n",
    "    # get the values of the Q-function at next state from the \"target\" network \n",
    "    # remember to detach, we need to treat these values as constants \n",
    "    q_target_next = target_q(next_states).detach()\n",
    "    \n",
    "    # get the max value \n",
    "    max_q_target_next = q_target_next.max(1)[0]\n",
    "\n",
    "    # one important step, often overlooked, is to ensure \n",
    "    # that failure states are grounded to zero\n",
    "    max_q_target_next *= (1 - failures.float())\n",
    "\n",
    "    # calculate the target \n",
    "    target = rewards + gamma * max_q_target_next\n",
    "\n",
    "    # finally, we get the current estimate of Q(s,a)\n",
    "    # here we query the current \"online\" network\n",
    "    q_online_current = torch.gather(online_q(states), 1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # create the errors\n",
    "    td_error = target - q_online_current\n",
    "\n",
    "    # calculate the loss\n",
    "    loss = td_error.pow(2).mean()\n",
    "\n",
    "    # backward pass: compute the gradients\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update model parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563835c2",
   "metadata": {},
   "source": [
    "## Exploration Vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c5115",
   "metadata": {},
   "source": [
    "Hyperparameters to be set for the exploration straetegies. *($\\epsilon$ decay)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39038a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define decay parameters (max, min, steps)\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay_steps = 10000\n",
    "\n",
    "# generate epsilons\n",
    "epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "   \n",
    "# normalize epsilons \n",
    "epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "    \n",
    "# scale  epsilons to the desired range\n",
    "epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c384cb",
   "metadata": {},
   "source": [
    "Chosen exploration strategy.\\\n",
    "Based on the picked $\\epsilon$, we alternate a random action selection and the greedy selection (greedy based on the value of the $Q(s,a,\\theta)$ returned by our neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28f0a2",
   "metadata": {},
   "source": [
    "`random_pi()` is the baseline random policy, to be used as reference, in order to understand whether we are actually learning smoething"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_pi(state):\n",
    "    # selects an action uniformly at random\n",
    "    # from the environment's action space.\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25acb4a",
   "metadata": {},
   "source": [
    "At the end of training the online network will be used to implement the policy.\\\n",
    "`dqn_policy()` it simply acts greedy given the input state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ca563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_pi(state):\n",
    "    # convert the state into a tensor\n",
    "    state = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "    # compute Q-values from the network\n",
    "    q_values = online_q(state).detach().numpy().squeeze()\n",
    "\n",
    "    # select greedy action\n",
    "    action = int(np.argmax(q_values))\n",
    "\n",
    "    # return the action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5ff48",
   "metadata": {},
   "source": [
    "`epsilon_greedy()` combines `random_pi()` and `dqn_pi()` in order to be indeed $\\epsilon\\$-greedy decaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9794a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, step):\n",
    "    # get the epsilon value    \n",
    "    epsilon = epsilons[step] if step < epsilon_decay_steps else epsilon_min\n",
    "\n",
    "    # Exploration\n",
    "    if random.random() < epsilon:\n",
    "        action = random_pi(state)\n",
    "\n",
    "    # Exploitation\n",
    "    else:\n",
    "        action = dqn_pi(state)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121af31",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "Function to evaluate a policy, it returns the average reward obtained over a number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pi, episodes=1):\n",
    "\n",
    "     # collect total rewards per episode\n",
    "    rewards = []\n",
    "\n",
    "    # loop over episodes\n",
    "    for episode in range(episodes):\n",
    "\n",
    "        # reset the environment\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # run an episode\n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            state, reward, terminal, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            done = terminal or truncated\n",
    "\n",
    "        # store the total reward    \n",
    "        rewards.append(total_reward)\n",
    "            \n",
    "    # return the average reward over the episodes        \n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c3170",
   "metadata": {},
   "source": [
    "## Deep Q-Network Algorithm\n",
    "Putting all the things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn(memory_start_size, target_update_steps, max_episodes):\n",
    "    \n",
    "    # create a score tracker for statistic purposes\n",
    "    scores = []\n",
    "    \n",
    "    # counter for the number of steps \n",
    "    step = 0\n",
    "\n",
    "    # update the target model with the online one\n",
    "    update_target()\n",
    "                   \n",
    "    # train until the maximum number of episodes\n",
    "    for episode in range(max_episodes):\n",
    "        \n",
    "        # reset the environment before starting the episode\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # interact with the environment until the episode is done\n",
    "        while not done:\n",
    "                    \n",
    "            # select the action using the exploration policy\n",
    "            action = epsilon_greedy(state, step)\n",
    "\n",
    "            # perform the selected action\n",
    "            next_state, reward, terminal, truncated, _ = env.step(action)\n",
    "            done = terminal or truncated\n",
    "            failure = terminal and not truncated\n",
    "\n",
    "            # store the experience into the replay buffer\n",
    "            experience = (state, action, reward, next_state, failure)\n",
    "            store_experience(experience)\n",
    "    \n",
    "            # optimize the online model after the replay buffer is large enough\n",
    "            if replay_memory['entries'] > memory_start_size:\n",
    "                optimize()\n",
    "                 \n",
    "                # sometimes, synchronize the target model with the online model\n",
    "                if step % target_update_steps == 0:\n",
    "                    update_target()\n",
    "                \n",
    "            # update current state to next state\n",
    "            state = next_state\n",
    "\n",
    "            # update the step counter\n",
    "            step += 1\n",
    "\n",
    "        # After each episode, evaluate the policy\n",
    "        score = evaluate(dqn_pi, episodes=10)\n",
    "\n",
    "        # store the score in the tracker\n",
    "        scores.append(score)\n",
    "\n",
    "        # print some informative logging \n",
    "        message = 'Episode {:03}, score {:05.1f}'\n",
    "        message = message.format(episode+1, score)\n",
    "        print(message, end='\\r', flush=True)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9c2f2",
   "metadata": {},
   "source": [
    "Apply the DQN to our `TradingEnv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c240f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "memory_start_size = 1000\n",
    "max_episodes = 100 #TODO! 100\n",
    "target_update_steps = 10\n",
    "\n",
    "# run the DQN algorithm\n",
    "dqn(memory_start_size, target_update_steps, max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706bff6",
   "metadata": {},
   "source": [
    "## Compare DQN to random_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda097f",
   "metadata": {},
   "source": [
    "Experiment to understand how dqn is learning over the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(max_episodes):\n",
    "\n",
    "    global online_q, target_q, optimizer, replay_memory, epsilons\n",
    "\n",
    "    # List of random seeds to test algorithm stability\n",
    "    seeds = (12, 34, 56, 78, 90)\n",
    "\n",
    "    # Container to collect all experiment results\n",
    "    results = []\n",
    "\n",
    "    # Run an independent training experiment per seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Experiment seed: \", seed)\n",
    "\n",
    "         # Set all relevant random seeds for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        # create online and target models\n",
    "        online_q = create_network()\n",
    "        target_q = create_network()\n",
    "        optimizer = torch.optim.RMSprop(online_q.parameters(), lr=learning_rate)\n",
    "\n",
    "        # create the replay memory\n",
    "        replay_memory = {\n",
    "            'size': memory_size,\n",
    "            'buffer': np.empty(shape=(memory_size,), dtype=experience_type),\n",
    "            'index': 0,\n",
    "            'entries': 0\n",
    "        }\n",
    "\n",
    "        # create the epsilon values\n",
    "        epsilons = np.logspace(start=0, stop=-2, num=epsilon_decay_steps, base=10)\n",
    "        epsilons = (epsilons - epsilon_min) / (epsilon_max - epsilon_min)\n",
    "        epsilons = (epsilon_max - epsilon_min) * epsilons + epsilon_min\n",
    "\n",
    "        # train the network    \n",
    "        scores = dqn(memory_start_size, target_update_steps, max_episodes)\n",
    "        \n",
    "        # smooth the result using a sliding window\n",
    "        sliding_windows = 25\n",
    "        scores = np.convolve(scores, np.ones(sliding_windows)/sliding_windows, mode='valid')\n",
    "                \n",
    "        # collect the results\n",
    "        results.append(scores)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # calculate max, min and average scores among experiments\n",
    "    max_score = np.max(results, axis=0).T\n",
    "    min_score = np.min(results, axis=0).T\n",
    "    mean_score = np.mean(results, axis=0).T\n",
    "\n",
    "    # prepare the results\n",
    "    experiment_results = {\n",
    "        'max_score': max_score,\n",
    "        'min_score': min_score,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "    # save permanently\n",
    "    joblib.dump(experiment_results, '../dqn_results.joblib');\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91ef7",
   "metadata": {},
   "source": [
    "Run of the experiment on our environemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb04ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the experiment setup and hyperparameters\n",
    "gamma = 0.99               # discount factor\n",
    "learning_rate = 0.001      # step size for the optimizer\n",
    "batch_size = 512          # number of experiences per batch\n",
    "epochs = 8                # optimization steps per batch\n",
    "epsilon = 0.5               # esploration vs exploitation parameter\n",
    "first_hidden_layer = 256   # size of the first hidden layer\n",
    "second_hidden_layer = 128  # size of the second hidden layer\n",
    "\n",
    "# Run the experiment\n",
    "dqn_results = experiment(max_episodes=1500) #TODO! 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc00480",
   "metadata": {},
   "source": [
    "Experiment to see the performance of a random behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_random(max_episodes):\n",
    "\n",
    "    # List of random seeds to test algorithm stability\n",
    "    seeds = (12, 34, 56, 78, 90)\n",
    "\n",
    "    # Container to collect all experiment results\n",
    "    results = []\n",
    "\n",
    "    # Run an independent training experiment per seed\n",
    "    for seed in seeds:\n",
    "\n",
    "        print(\"Experiment seed: \", seed)\n",
    "\n",
    "         # Set all relevant random seeds for reproducibility\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        # reset the environment\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        # train the network    \n",
    "        scores = evaluate(random_pi, episodes=max_episodes)\n",
    "        \n",
    "        # smooth the result using a sliding window\n",
    "        sliding_windows = 25\n",
    "        scores = np.convolve(scores, np.ones(sliding_windows)/sliding_windows, mode='valid')\n",
    "                \n",
    "        # collect the results\n",
    "        results.append(scores)\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # calculate max, min and average scores among experiments\n",
    "    max_score = np.max(results, axis=0).T\n",
    "    min_score = np.min(results, axis=0).T\n",
    "    mean_score = np.mean(results, axis=0).T\n",
    "\n",
    "    # prepare the results\n",
    "    experiment_results = {\n",
    "        'max_score': max_score,\n",
    "        'min_score': min_score,\n",
    "        'mean_score': mean_score\n",
    "    }\n",
    "\n",
    "    \n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f449c92",
   "metadata": {},
   "source": [
    "Run the experiment for the randomic behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pi_results = experiment_random(max_episodes=1500) #TODO! 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c814b3",
   "metadata": {},
   "source": [
    "### Comparison between DQN and random_pi in learning in terms of robustness with confidence bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cd4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the plots\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('DQN vs Random Policy')\n",
    "plt.ylabel('Average reward per episode [$]')\n",
    "plt.xlabel('Episodes')\n",
    "\n",
    "#get the results of the 2 experiments\n",
    "dqn_episodes = range(len(dqn_results['max_score']))\n",
    "random_pi_episodes = range(len(random_pi_results['max_score']))\n",
    "\n",
    "plt.plot(random_pi_results['max_score'], 'y', linewidth=1, label=\"random_pi\")\n",
    "plt.plot(random_pi_results['min_score'], 'y', linewidth=1)\n",
    "plt.plot(random_pi_results['mean_score'], 'y', linewidth=2)\n",
    "plt.fill_between(random_pi_episodes, random_pi_results['min_score'], random_pi_results['max_score'], facecolor='y', alpha=0.3)\n",
    "\n",
    "plt.plot(dqn_results['max_score'], 'b', linewidth=1, label=\"DQN\")\n",
    "plt.plot(dqn_results['min_score'], 'b', linewidth=1)\n",
    "plt.plot(dqn_results['mean_score'], 'b', linewidth=2)\n",
    "plt.fill_between(dqn_episodes, dqn_results['min_score'], dqn_results['max_score'], facecolor='b', alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d43945",
   "metadata": {},
   "source": [
    "## What our agent is capable of in reality\n",
    "\n",
    "It is crucial to clarify a fundamental aspect of reinforcement learning applied to financial trading, we are somehow *in the middle of reinforcement learning and supervised learning problem*.\\\n",
    "While the problem is formally framed as a reinforcement learning task, it exhibits several characteristics that make it closer to an offline and supervised-like learning setting, as in between a problem of pattern recognition.\n",
    "\n",
    "In particular, the actions taken by the agent do not influence the environment dynamics: the price time series is pre-generated and entirely exogenous. As a consequence, the environment is non-reactive, and the agent interacts with a fixed historical trajectory rather than a fully interactive Markov Decision Process. <small>(see the final MarkDown cell discussing Markovian assumptions)</small>\\\n",
    "[See the fiancial market whale](https://fenefx.com/en/blog/what-is-a-financial-market-whale/)\n",
    "\n",
    "This property introduces a significant risk of overfitting. Evaluating the cumulative reward obtained by the agent on the same time series used during training does not provide meaningful information about its true generalization capability. In such a setting, the agent may simply learn spurious correlations or regime-specific patterns present in the training data, achieving high in-sample performance without learning a robust trading strategy.\n",
    "\n",
    "Therefore, performance evaluation must be conducted on strictly unseen time series, either from future time periods or from different assets, in order to assess the agent’s ability to generalize to a real and previously unobserved market environment. Only out-of-sample evaluation can provide a realistic estimate of the strategy’s effectiveness and risk profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9ccb14",
   "metadata": {},
   "source": [
    "Thus, we will evaluate the performance of our agent over a shifted and shorter environment, **over the same market**, this last thing it is fundamental again.\\\n",
    "We should test our policy on the same market over which it has been trained on, because different markets might have different types of characteristics, such as: market cap, amount of investors, regulatory rules, volatility..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dec705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a new environment, we will use it for testing the trained policy\n",
    "#choose a shorter start_date - end_date range to speed up the evaluation phase\n",
    "env = TradingEnv(ticker=\"AAPL\", granularity=\"1d\", sliding_window=10, start_date=\"2025-01-01\", end_date=\"2026-01-01\", initial_cash=100000, initial_share=50)\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State Space: ', state_size)\n",
    "print('Action Space: ', action_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc9b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of the TradingEnv class to show the data used testing the trained policy\n",
    "env.show_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ec467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can even run it many times with many different input settings to draw a Design of Experiments curve\n",
    "evaluate(dqn_pi, episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just for comparison test the perfomance of the random policy\n",
    "evaluate(random_pi, episodes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d911f9",
   "metadata": {},
   "source": [
    "## Conclusions, Limitations and Future Improvements\n",
    "\n",
    "The results highlight a clear difference between a **random policy** and the policy learned by the DQN agent.  \n",
    "This suggests that the agent is able to capture **non-trivial patterns** in market dynamics, potentially exploiting regularities that go beyond pure randomness. However, these results should be interpreted with caution, as financial markets are complex and highly noisy systems.\n",
    "\n",
    "It is well known in the financial literature that markets may exhibit recurring structures, such as [CandleStick patterns](https://en.wikipedia.org/wiki/Candlestick_pattern), which reflect collective trader behavior and market psychology. Reinforcement learning methods may implicitly learn such patterns without explicitly encoding them, like in pattern recognition.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Despite the encouraging results, the proposed model suffers from several limitations:\n",
    "\n",
    "- **Strong assumptions on the action space**:  \n",
    "  The agent can only buy, sell, or hold a single unit of the asset at each step. This is a restrictive assumption that does not reflect real trading behavior.\n",
    "\n",
    "- **Simplified state representation**:  \n",
    "  The observation space is limited to a fixed-size sliding window of past price variations, excluding other potentially informative features such as volume, volatility indicators, or macroeconomic signals.\n",
    "\n",
    "- **Sensitivity to hyperparameters**:  \n",
    "  The performance of the model strongly depends on the choice of neural network architecture and DQN hyperparameters, which have not been exhaustively optimized.\\\n",
    "  Furthermore even the environment is strongly dependent on hyperparamters, that should be properly defined by a deep financial knowledge.\\\n",
    "  Even the initialization of the cash and share hyperparameters is crucial, because from this, the agent could learn degenerative behaviours.\n",
    "\n",
    "- **Limited time horizon and computational constraints**:  \n",
    "  Computational power limits the amount of historical data and the length of training, potentially preventing the agent from learning long-term dependencies.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "Several directions can be explored to improve the model:\n",
    "\n",
    "- **Hyperparameter optimization**:  \n",
    "  Perform a systematic search (e.g. grid search or random search) over network architectures and learning parameters.\n",
    "\n",
    "- **Different time resolutions**:  \n",
    "  Extend the framework to different time scales, such as intraday or hourly trading, to analyze high-frequency dynamics.\n",
    "\n",
    "- **Richer action space**:  \n",
    "  Allow the agent to buy or sell multiple units per action, enabling position sizing and more realistic portfolio management.\n",
    "\n",
    "- **Expanded state space**:  \n",
    "  Incorporate additional features such as technical indicators, trading volume, volatility measures, or external signals.\n",
    "\n",
    "- **Real-world deployment**:  \n",
    "  After proper validation and risk management, the trained agent could be connected to a trading bot interfaced with a real (or paper) trading wallet.\n",
    "\n",
    "- **Use of finer models**:  \n",
    "  We could try to use different models, such as DDQN, Rainbow or even policy-based methods.\n",
    "\n",
    "Overall, while the results are promising, further experimentation and validation are required before considering real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae860d",
   "metadata": {},
   "source": [
    "### On the Markov Decision Process formulation in financial trading\n",
    "\n",
    "A Markov Decision Process (MDP) is defined by a tuple $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$, where the transition function $P(s_{t+1} \\mid s_t, a_t)$ explicitly depends on the action taken by the agent.\\\n",
    "This dependency implies that actions actively shape the future evolution of the environment.\n",
    "\n",
    "In the context of financial trading on historical price data, this assumption does not strictly hold.\\\n",
    "The price time series is pre-generated and entirely exogenous, meaning that the agent’s actions do not influence the future market states.\n",
    "As a consequence, the state transition dynamics satisfy:\n",
    "\n",
    "$P(s_{t+1} \\mid s_t, a_t) = P(s_{t+1} \\mid s_t)$\n",
    "\n",
    "The agent’s actions affect only the reward function, through realized profits and losses, while the environment evolution remains unchanged.\\\n",
    "This results in a non-reactive environment, where the agent interacts with a fixed historical trajectory rather than a fully interactive MDP.\n",
    "\n",
    "Therefore, the trading problem is more accurately described as an offline reinforcement learning problem with exogenous dynamics, or as a degenerate MDP.\\\n",
    "This formulation highlights the increased risk of overfitting and the necessity of strict out-of-sample evaluation, as the agent may otherwise exploit idiosyncratic patterns specific to the training data rather than learning a robust trading policy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
